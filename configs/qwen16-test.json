{
  "model_name": "unsloth/Qwen3-1.7B",
  "display_name": null,
  "lora_rank": 16,
  "lora_alpha": 32,
  "lora_dropout": 0.05,
  "lora_target_modules": [
    "q_proj",
    "v_proj"
  ],
  "lora_bias": "none",
  "dataset_source": "huggingface",
  "dataset_path": "openai/gsm8k",
  "dataset_config": "main",
  "dataset_split": "train[:100]",
  "instruction_field": "question",
  "response_field": "answer",
  "reasoning_start": "<start_working_out>",
  "reasoning_end": "<end_working_out>",
  "solution_start": "<SOLUTION>",
  "solution_end": "</SOLUTION>",
  "system_prompt": "You are given a problem.\nThink about the problem and provide your working out.\nPlace it between <start_working_out> and <end_working_out>.\nThen, provide your solution between <SOLUTION></SOLUTION>",
  "chat_template_type": "grpo",
  "chat_template": "{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] + eos_token }}{% set loop_messages = messages[1:] %}{% else %}{{ system_prompt + eos_token }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ message['content'] }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ reasoning_start }}{% endif %}",
  "enable_pre_training": true,
  "pre_training_epochs": 1,
  "pre_training_samples": 25,
  "validate_format": true,
  "learning_rate": 0.0001,
  "batch_size": 2,
  "num_epochs": 5,
  "gradient_accumulation_steps": 1,
  "max_sequence_length": 2048,
  "max_new_tokens": 256,
  "warmup_steps": 10,
  "weight_decay": 0.001,
  "temperature": 0.5,
  "top_p": 0.95,
  "top_k": 50,
  "repetition_penalty": 1,
  "kl_penalty": 0.01,
  "clip_range": 0.2,
  "num_generations": 2,
  "value_coefficient": 1,
  "loss_type": "grpo",
  "importance_sampling_level": "sequence",
  "epsilon": 0.0003,
  "epsilon_high": 0.0004,
  "lr_scheduler_type": "constant",
  "optim": "paged_adamw_32bit",
  "max_grad_norm": 0.3,
  "logging_steps": 10,
  "save_steps": 100,
  "eval_steps": 100,
  "seed": 42,
  "use_4bit": false,
  "use_8bit": false,
  "bnb_4bit_compute_dtype": "float16",
  "bnb_4bit_quant_type": "nf4",
  "use_nested_quant": false,
  "reward_config": {
    "type": "preset",
    "preset": "math"
  },
  "use_flash_attention": false,
  "gradient_checkpointing": false,
  "fp16": true,
  "bf16": false,
  "filename": "qwen16-test.json"
}